{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "H5_Q8HAabyDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dxX_TZi_7PA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4gmSmAw6BoF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "images = np.load('/content/drive/MyDrive/Datasets/UDIAT/Fused/images.npy')\n",
        "labels = np.load('/content/drive/MyDrive/Datasets/UDIAT/Fused/labels.npy')\n",
        "classes = [\"Benign\", \"Malignant\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the ratios for splitting\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.1\n",
        "test_ratio = 0.2\n",
        "\n",
        "# Split the data into training, validation, and test sets\n",
        "x_train, x_temp, y_train, y_temp = train_test_split(\n",
        "    images, labels, test_size=1 - train_ratio, random_state=42)\n",
        "\n",
        "x_val, x_test, y_val, y_test = train_test_split(\n",
        "    x_temp, y_temp, test_size=test_ratio / (test_ratio + val_ratio), random_state=42)"
      ],
      "metadata": {
        "id": "NM0N_ERnD6vR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of samples in each set\n",
        "print(f\"Number of samples in training set: {len(x_train)}\")\n",
        "print(f\"Number of samples in validation set: {len(x_val)}\")\n",
        "print(f\"Number of samples in test set: {len(x_test)}\")"
      ],
      "metadata": {
        "id": "L5mrRNBtD_DD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of samples in each class within each set\n",
        "for set_name, x_set, y_set in [(\"Training\", x_train, y_train), (\"Validation\", x_val, y_val), (\"Test\", x_test, y_test)]:\n",
        "    print(f\"Samples in {set_name} set:\")\n",
        "    for class_label, class_name in enumerate(classes):\n",
        "        num_samples = np.sum(y_set == class_label)\n",
        "        print(f\"  Class '{class_name}': {num_samples}\")\n"
      ],
      "metadata": {
        "id": "MwNKh3mAEGDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "# Use LabelBinarizer for one-hot encoding\n",
        "label_binarizer = LabelBinarizer()\n",
        "y_train = label_binarizer.fit_transform(y_train)\n",
        "y_val = label_binarizer.fit_transform(y_val)\n",
        "y_test = label_binarizer.fit_transform(y_test)"
      ],
      "metadata": {
        "id": "x2MFKq8TEtW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of samples in each set\n",
        "print(f\"Shape of labels in training set: {np.shape(y_train)}\")\n",
        "print(f\"Shape of labels in validation set: {np.shape(y_val)}\")\n",
        "print(f\"Shape of labels in test set: {np.shape(y_test)}\")"
      ],
      "metadata": {
        "id": "xzVCMmrKEghE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot few random samples"
      ],
      "metadata": {
        "id": "ufEP8QIqds7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to plot random samples of benign and malignant images\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_random_samples(images, labels, class_name, num_samples=3):\n",
        "    # Get indices of images with the specified class\n",
        "    class_indices = np.where(labels == class_name)[0]\n",
        "\n",
        "    # Randomly select num_samples indices\n",
        "    random_indices = np.random.choice(class_indices, num_samples, replace=False)\n",
        "\n",
        "    # Create a subplots grid\n",
        "    fig, axes = plt.subplots(1, num_samples, figsize=(6, 4))\n",
        "\n",
        "    for i, idx in enumerate(random_indices):\n",
        "        ax = axes[i]\n",
        "        ax.imshow(images[idx])\n",
        "        ax.set_title(f'Sample {i+1}')\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "E10LLD_Jdr5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot 3 random samples of benign images from training data\n",
        "plot_random_samples(images, labels, class_name=0, num_samples=3)"
      ],
      "metadata": {
        "id": "o3GGKpx-d1Ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot 3 random samples of malignant images from training data\n",
        "plot_random_samples(images, labels, class_name=1, num_samples=3)"
      ],
      "metadata": {
        "id": "FXVb_LKxe1DX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility Functions"
      ],
      "metadata": {
        "id": "Jw-J9yTYzBTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Libraries\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn import metrics\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Ix3xGwunzJej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the multilayer perceptron classifier\n",
        "mlp = MLPClassifier()\n",
        "\n",
        "# Initialize the k-nearest neighbors classifier\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Initialize the random forest classifier\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "# Initialize the gradient boosting classifier\n",
        "gb = GradientBoostingClassifier()\n",
        "\n",
        "# Create a voting classifier with weights for the individual classifiers\n",
        "clf = VotingClassifier(\n",
        "    estimators=[('mlp', mlp), ('knn', knn), ('rf', rf), ('gb', gb)],\n",
        "    weights=[1, 1, 2, 1], voting='soft')\n",
        "\n",
        "\n",
        "classifiers = ['RandomForest',\n",
        "               'AdaBoost',  'DecisionTree',\n",
        "               'KNeighbors','MLP','GradientBoosting', 'Proposed']\n",
        "models = [RandomForestClassifier(n_estimators=200, random_state=0),\n",
        "          AdaBoostClassifier(random_state = 0),\n",
        "          DecisionTreeClassifier(random_state=0),\n",
        "          KNeighborsClassifier(),\n",
        "          MLPClassifier(hidden_layer_sizes=(150,100,50), max_iter=300,activation = 'relu',solver='adam',random_state=1),\n",
        "          GradientBoostingClassifier(random_state=0),\n",
        "          clf]"
      ],
      "metadata": {
        "id": "QCDD3P12zTSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sensitivity and specificity functions\n",
        "def sensitivity(y_true, y_pred):\n",
        "    tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_pred).ravel()\n",
        "    return tp / (tp + fn)\n",
        "\n",
        "def specificity(y_true, y_pred):\n",
        "    tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_pred).ravel()\n",
        "    return tn / (tn + fp)\n",
        "\n",
        "# Score of classifiers\n",
        "def acc_score(X_train,X_test,Y_train,y_true):\n",
        "    Score = pd.DataFrame({\"Classifier\":classifiers})\n",
        "\n",
        "    acc = []\n",
        "    pr = []\n",
        "    re = []\n",
        "    f1 = []\n",
        "    auc = []\n",
        "    spec = []\n",
        "    sen = []\n",
        "\n",
        "    for i in models:\n",
        "        model = i\n",
        "        model.fit(X_train,Y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        acc.append(metrics.accuracy_score(y_true, y_pred))\n",
        "        pr.append(metrics.precision_score(y_true, y_pred))\n",
        "        re.append(metrics.recall_score(y_true, y_pred))\n",
        "        f1.append(metrics.f1_score(y_true, y_pred))\n",
        "        area = metrics.roc_auc_score(y_true, model.predict_proba(X_test)[:, 1])\n",
        "        auc.append(area)\n",
        "        sen.append(sensitivity(y_true, y_pred))\n",
        "        spec.append(specificity(y_true, y_pred))\n",
        "\n",
        "    Score[\"Accuracy\"] = acc\n",
        "    Score[\"Precision\"] = pr\n",
        "    Score[\"Recall\"] = re\n",
        "    Score[\"Sensitivity\"] = sen\n",
        "    Score[\"Specificity\"] = spec\n",
        "    Score[\"F1_Score\"] = f1\n",
        "    Score[\"ROC_AUC\"] = auc\n",
        "\n",
        "    Score.sort_values(by=\"F1_Score\", ascending=False,inplace = True)\n",
        "    Score.reset_index(drop=True, inplace=True)\n",
        "    return Score\n"
      ],
      "metadata": {
        "id": "xt5aV-bkzA0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ablation Study"
      ],
      "metadata": {
        "id": "8tH5ndVUtlSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification with CNN Model"
      ],
      "metadata": {
        "id": "a_FhFn_YtzG4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build and Compile Model"
      ],
      "metadata": {
        "id": "APUdufSEOJp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications import MobileNet\n",
        "import tensorflow as tf\n",
        "\n",
        "# MobileNet Model\n",
        "def create_model(SIZE):\n",
        "  model =  MobileNet(weights='imagenet', include_top=False, input_shape=(SIZE, SIZE, 3))\n",
        "  stringlist = []\n",
        "  model.summary(print_fn=lambda x: stringlist.append(x))\n",
        "  short_model_summary = \"\\n\".join(stringlist)\n",
        "  print(short_model_summary)\n",
        "  for layer in model.layers:\n",
        "    layer.trainable = False\n",
        "  # Flatten the output layer to 1 dimension\n",
        "  x = tf.keras.layers.Flatten()(model.output)\n",
        "\n",
        "  # Add a fully connected layer with 512 hidden units and ReLU activation\n",
        "  x = tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
        "\n",
        "  # Add a dropout rate of 0.3\n",
        "  x = tf.keras.layers.Dropout(0.3)(x)\n",
        "\n",
        "  # Add a batch normalization layer\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "  x = tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
        "\n",
        "  # Add a dropout rate of 0.2\n",
        "  x = tf.keras.layers.Dropout(0.2)(x)\n",
        "\n",
        "  # Add a batch normalization layer\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "  # Add a final sigmoid layer for classification\n",
        "  output = tf.keras.layers.Dense(1 , activation='sigmoid')(x)\n",
        "\n",
        "  model = tf.keras.Model(model.input, output)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "hYDywZ5B7Lf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_history(history=None):\n",
        "    train_loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    train_acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "\n",
        "    epochsn = np.arange(1, len(train_loss)+1)\n",
        "    plt.figure(figsize = (20,7))\n",
        "\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.plot(epochsn,train_loss, 'b', label='Training Loss')\n",
        "    plt.plot(epochsn,val_loss, 'r', label='Validation Loss')\n",
        "    plt.grid(color='gray', linestyle='--')\n",
        "    plt.legend()\n",
        "    plt.title('Training Loss vs Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.plot(epochsn, train_acc, 'b', label='Training Accuracy')\n",
        "    plt.plot(epochsn, val_acc, 'r', label='Validation Accuracy')\n",
        "    plt.grid(color='gray', linestyle='--')\n",
        "    plt.legend()\n",
        "    plt.title('Training Accuracy vs Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.savefig(\"Learning_Curves.png\", dpi=300)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "qnW4T5XA78W0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the final model\n",
        "SIZE = 224\n",
        "model = create_model(SIZE)"
      ],
      "metadata": {
        "id": "7Lk3RABs7JV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add early stopping and reduce learning rate callbacks\n",
        "mc = tf.keras.callbacks.ModelCheckpoint(\"trained_model.h5\", monitor='val_accuracy', mode='max',verbose=1, save_best_only=True)\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, restore_best_weights=True, patience=20)\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)"
      ],
      "metadata": {
        "id": "b3YYlp_h8K8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the Model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Aj2Qj8TSKGR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stringlist = []\n",
        "model.summary(print_fn=lambda x: stringlist.append(x))\n",
        "short_model_summary = \"\\n\".join(stringlist)\n",
        "print(short_model_summary)"
      ],
      "metadata": {
        "id": "HZFoi2yoHiUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Model"
      ],
      "metadata": {
        "id": "Wtw1YoZgOFqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply some augmentation to control overfitting\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(rotation_range=15, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
        "datagen.fit(images)\n",
        "\n",
        "train_generator = datagen.flow(x=x_train, y=y_train, batch_size=32)"
      ],
      "metadata": {
        "id": "LVru0eKXfbL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Model\n",
        "from datetime import datetime\n",
        "start = datetime.now()\n",
        "model_history = model.fit(train_generator, steps_per_epoch=len(x_train) // 32, batch_size=32, epochs=200, validation_data=(x_val, y_val), callbacks=[es, mc, reduce_lr])\n",
        "stop = datetime.now()"
      ],
      "metadata": {
        "id": "qpZQn5wp8Mag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the Training Time\n",
        "\n",
        "training_time = stop - start\n",
        "print('Model training time is :', training_time)"
      ],
      "metadata": {
        "id": "MAgGgZh_KnwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the history\n",
        "plot_history(model_history)"
      ],
      "metadata": {
        "id": "7DGTD-1a8udN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Model on Test data"
      ],
      "metadata": {
        "id": "R8hmUupS8_xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "model = tf.keras.models.load_model('/content/trained_model.h5')\n",
        "y_pred = model.predict(x_test)"
      ],
      "metadata": {
        "id": "akui5KL-9r4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate scores\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "accuracy = accuracy_score(y_test, y_pred.round())\n",
        "sensitivity = recall_score(y_test, y_pred.round())\n",
        "specificity = recall_score(y_test, y_pred.round(), pos_label=0)\n",
        "f1 = f1_score(y_test, y_pred.round())\n",
        "roc = roc_auc_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "tIyQ-qt3-E5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the results\n",
        "print(\"scores\")\n",
        "print(\"==================================================\")\n",
        "print(\"Accuracy score: %.4f\" % (accuracy))\n",
        "print(\"Sensitivity score: %.4f\" % (sensitivity))\n",
        "print(\"Specificity score: %.4f\" % (specificity))\n",
        "print(\"F1 score: %.4f\" % (f1))\n",
        "print(\"roc_auc score: %.4f\" % (roc))\n",
        "print(\"==================================================\")"
      ],
      "metadata": {
        "id": "MUNQ2SnIr64X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "\n",
        "    # Create a confusion matrix\n",
        "    y_pred_binary = y_pred.round()\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred_binary)\n",
        "\n",
        "    # Plot the confusion matrix as a heatmap\n",
        "    plt.imshow(cm, cmap=plt.cm.Blues)\n",
        "    plt.ylabel('True label', fontsize=14)\n",
        "    plt.xlabel('Predicted label', fontsize=14)\n",
        "    plt.xticks([0, 1], ['Benign', 'Malignant'], fontsize=14)\n",
        "    plt.yticks([0, 1], ['Benign', 'Malignant'], fontsize=14)\n",
        "    plt.colorbar()\n",
        "\n",
        "    # Add the values inside the cells\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            plt.text(j, i, cm[i, j],\n",
        "                     fontsize=20,  # specify the desired font size\n",
        "                     horizontalalignment='center',\n",
        "                     color='black')\n",
        "    # Show the plot\n",
        "    plt.savefig(\"Confusion_Matrix.png\", dpi=300)\n",
        "    plt.show()\n",
        "plot_confusion_matrix(y_test, y_pred)"
      ],
      "metadata": {
        "id": "5eucDROA-UEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot ROC Curve\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "def plot_roc_curve(y_true, y_pred):\n",
        "    # Calculate the false positive rate and true positive rate for the ROC curve\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Plot the ROC curve\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.3f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([-0.01, 1.0])\n",
        "    plt.ylim([0, 1.05])\n",
        "    plt.xlabel('False Positive Rate', fontsize=14)\n",
        "    plt.ylabel('True Positive Rate', fontsize=14)\n",
        "    plt.title('Receiver operating characteristic', fontsize=14)\n",
        "    plt.legend(loc=\"lower right\", fontsize=14)\n",
        "    plt.savefig(\"ROC_Curve.png\", dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "plot_roc_curve(y_test, y_pred)"
      ],
      "metadata": {
        "id": "IuP_jiPe_BVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction"
      ],
      "metadata": {
        "id": "4sKs2bEZN9mm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained model\n",
        "model = tf.keras.models.load_model('/content/trained_model.h5')\n",
        "\n",
        "# Get the total number of layers in the model\n",
        "total_layers = len(model.layers)\n",
        "print(\"Total Layers in the Model are: \", total_layers)\n",
        "\n",
        "# Define the index of the layer for feature extraction (second-to-last layer in this case)\n",
        "feature_extraction_layer_index = total_layers - 7\n",
        "\n",
        "# Get the name of the feature extraction layer\n",
        "feature_extraction_layer_name = model.layers[feature_extraction_layer_index].name\n",
        "print(\"Name of the Feature Extraction Layer: \", feature_extraction_layer_name)\n",
        "\n",
        "\n",
        "# Specify the layers from which you want to extract features\n",
        "feature_extraction_model = tf.keras.Model(inputs=model.input, outputs=model.get_layer(feature_extraction_layer_name).output)\n",
        "\n",
        "print(\"Extracting features from images...................\")\n",
        "# Extract features from images\n",
        "features = feature_extraction_model.predict(images)"
      ],
      "metadata": {
        "id": "lonXT3xmN8bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape of the feature arrays\n",
        "print(\"Shape of features:\", features.shape)"
      ],
      "metadata": {
        "id": "yhLUaiAaRHRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the feature arrays as numpy arrays\n",
        "np.save('features.npy', features)"
      ],
      "metadata": {
        "id": "VQqQXylARcYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize Using t-SNE"
      ],
      "metadata": {
        "id": "Pvo20asBguXw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply t-SNE to reduce the dimensionality of your extracted features."
      ],
      "metadata": {
        "id": "rs1s3wQDg9UY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Create a t-SNE model with the desired number of dimensions (e.g., 2 for 2D visualization)\n",
        "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
        "\n",
        "# Fit the t-SNE model to the features and transform them to the lower-dimensional space\n",
        "reduced_features = tsne.fit_transform(features)\n"
      ],
      "metadata": {
        "id": "ZlmJeaGsgt9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjust the perplexity parameter to control the balance between preserving global and local structure in the visualization. You may need to experiment with different values to find the most suitable one for your data."
      ],
      "metadata": {
        "id": "zSaidhs1hCvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming test_labels contains the true labels (0 for Benign, 1 for Malignant)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.scatter(reduced_features[labels == 0, 0], reduced_features[labels == 0, 1], label='Benign', alpha=0.5)\n",
        "plt.scatter(reduced_features[labels == 1, 0], reduced_features[labels == 1, 1], label='Malignant', alpha=0.5)\n",
        "plt.scatter(reduced_features[labels == 2, 0], reduced_features[labels == 2, 1], label='Normal', alpha=0.5)\n",
        "plt.legend()\n",
        "plt.title('t-SNE Visualization of Extracted Features')\n",
        "#plt.xlabel('t-SNE Component 1')\n",
        "#plt.ylabel('t-SNE Component 2')\n",
        "plt.axis(\"off\")\n",
        "plt.savefig(\"tSNE_Visual.png\", dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ANYRViCXhFmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification with Ensemble Model after Feature Extraction using CNN"
      ],
      "metadata": {
        "id": "uCwq9PIjuDOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load Training data\n",
        "features = np.load('/content/features.npy')\n",
        "labels = np.load('/content/drive/MyDrive/Datasets/UDIAT/Fused/labels.npy')\n",
        "classes = [\"Benign\", \"Malignant\"]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
        "\n",
        "# Print the number of samples in each set\n",
        "print(f\"Shape of training set: {np.shape(train_features)}\")\n",
        "print(f\"Shape of test set: {np.shape(test_features)}\")\n",
        "print(f\"Shape of labels in training set: {np.shape(train_labels)}\")\n",
        "print(f\"Shape of labels in test set: {np.shape(test_labels)}\")\n",
        "\n",
        "# Print the number of samples in each class within each set\n",
        "for set_name, x_set, y_set in [(\"Training\", train_features, train_labels), (\"Test\", test_features, test_labels)]:\n",
        "    print(f\"Samples in {set_name} set:\")\n",
        "    for class_label, class_name in enumerate(classes):\n",
        "        num_samples = np.sum(y_set == class_label)\n",
        "        print(f\"  Class '{class_name}': {num_samples}\")\n",
        "\n",
        "############################################################################"
      ],
      "metadata": {
        "id": "JeDOJXxNuRig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate\n",
        "scores = acc_score(train_features, test_features, train_labels, test_labels)"
      ],
      "metadata": {
        "id": "itTIV56k0e8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sorting by column 'Classifier'\n",
        "sorted_scores = scores.sort_values(by=['F1_Score'], ascending=False)\n",
        "print(\"Results on test data\")\n",
        "print(\"-----------------------------\")\n",
        "sorted_scores.head(10)"
      ],
      "metadata": {
        "id": "x5ASVDus0hAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_scores.to_csv(\"sorted_score_feature_results_UDIAT.csv\")"
      ],
      "metadata": {
        "id": "JhEgpLWq0i3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification with Ensemble Model after Feature Extraction using CNN and Applying GA"
      ],
      "metadata": {
        "id": "jpRJPz_P38n3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Genetic ALgorithm Functions\n",
        "\n",
        "import random\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_validate\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "# Constants\n",
        "POPULATION_SIZE = 30\n",
        "MAX_GENERATIONS = 50\n",
        "\n",
        "def initialize_population(num_features, population_size):\n",
        "    population = []\n",
        "    for _ in range(population_size):\n",
        "        solution = random.sample(range(num_features), random.randint(1, num_features))\n",
        "        population.append(solution)\n",
        "    return population\n",
        "\n",
        "\n",
        "def roulette_wheel_selection(population, fitnesses):\n",
        "    # Normalize fitness values\n",
        "    total_fitness = sum(fitnesses)\n",
        "    normalized_fitnesses = [f/total_fitness for f in fitnesses]\n",
        "\n",
        "    # Generate roulette wheel\n",
        "    wheel = []\n",
        "    current_position = 0\n",
        "    for f in normalized_fitnesses:\n",
        "        current_position += f\n",
        "        wheel.append(current_position)\n",
        "\n",
        "    # Select parents\n",
        "    parents = []\n",
        "    for i in range(len(population)):\n",
        "        r = random.random()\n",
        "        for j, w in enumerate(wheel):\n",
        "            if r <= w:\n",
        "                parents.append(population[j])\n",
        "                break\n",
        "\n",
        "    return parents\n",
        "\n",
        "\n",
        "def two_point_crossover(parent1, parent2):\n",
        "    # Choose crossover points\n",
        "    point1 = random.randint(0, len(parent1))\n",
        "    point2 = random.randint(point1, len(parent1))\n",
        "\n",
        "    # Create offspring\n",
        "    child1 = parent1[:point1] + parent2[point1:point2] + parent1[point2:]\n",
        "    child2 = parent2[:point1] + parent1[point1:point2] + parent2[point2:]\n",
        "\n",
        "    return child1, child2\n",
        "\n",
        "\n",
        "\n",
        "def mutate(solution, num_features, mutation_rate):\n",
        "    # Determine whether to add or remove a feature\n",
        "    if random.random() < mutation_rate:\n",
        "        # Add a random feature\n",
        "        solution.append(random.randint(0, num_features-1))\n",
        "    elif len(solution) > 1:\n",
        "        # Remove a random feature\n",
        "        solution.remove(random.choice(solution))\n",
        "    return solution\n",
        "\n",
        "\n",
        "\n",
        "def select_fittest(population, fitnesses):\n",
        "    # Sort population and fitnesses by fitness values\n",
        "    population_fitnesses = list(zip(population, fitnesses))\n",
        "    population_fitnesses.sort(key=lambda x: x[1], reverse=True)\n",
        "    sorted_population, sorted_fitnesses = zip(*population_fitnesses)\n",
        "\n",
        "    # # Return sorted population up to the maximum size\n",
        "    return sorted_population[:POPULATION_SIZE]\n",
        "\n",
        "\n",
        "def evaluate_fitness(population):\n",
        "    fitnesses = []\n",
        "    for solution in population:\n",
        "        model = RandomForestClassifier()\n",
        "        scores = cross_validate(model, X[:, solution], y, cv=5, scoring='f1')\n",
        "        fitnesses.append(scores['test_score'].mean())\n",
        "    return fitnesses\n",
        "\n",
        "\n",
        "def optimize_feature_subset(X, y):\n",
        "    # Initialize population\n",
        "    population = initialize_population(X.shape[1], POPULATION_SIZE)\n",
        "    fitnesses = evaluate_fitness(population)\n",
        "    all_fitnesses = [fitnesses] # store fitness values in all generations\n",
        "\n",
        "    previous_max_fitness = -float(\"inf\") # minimum possible fitness value\n",
        "    no_progress_count = 0 # counter for number of generations with no progress\n",
        "\n",
        "    for generation in tqdm(range(MAX_GENERATIONS)):\n",
        "        #print(f\"Generation {generation}: population size = {len(population)}, fitnesses size = {len(fitnesses)}\")\n",
        "        #print(f\"Population: {population}\")\n",
        "        #print(f\"Fitnesses: {fitnesses}\")\n",
        "\n",
        "        # Select fittest solutions for next generation\n",
        "        population = select_fittest(population, fitnesses)\n",
        "\n",
        "        # Generate offspring through crossover and mutation\n",
        "        offspring = []\n",
        "        for _ in range(len(population)//2):\n",
        "            parent1, parent2 = random.sample(population, 2)\n",
        "            child1, child2 = two_point_crossover(parent1, parent2)\n",
        "            child1 = mutate(child1, X.shape[1], 1/X.shape[1])\n",
        "            child2 = mutate(child2, X.shape[1], 1/X.shape[1])\n",
        "            offspring.extend([child1, child2])\n",
        "\n",
        "        # Evaluate fitness of offspring\n",
        "        offspring_fitnesses = evaluate_fitness(offspring)\n",
        "\n",
        "        # Select fittest solutions for next generation\n",
        "        population = select_fittest(list(population) + offspring, fitnesses + offspring_fitnesses)\n",
        "\n",
        "\n",
        "        # Evaluate fitness of current population\n",
        "        fitnesses = evaluate_fitness(population)\n",
        "        all_fitnesses.append(fitnesses)\n",
        "\n",
        "        # Check for progress\n",
        "        current_max_fitness = max(fitnesses)\n",
        "        if current_max_fitness <= previous_max_fitness:\n",
        "            no_progress_count += 1\n",
        "        else:\n",
        "            no_progress_count = 0\n",
        "            previous_max_fitness = current_max_fitness\n",
        "\n",
        "        # Terminate if no progress for 5 generations\n",
        "        if no_progress_count >= 5:\n",
        "            break\n",
        "\n",
        "    # Return fittest solution\n",
        "    fittest_index = fitnesses.index(max(fitnesses))\n",
        "    return population[fittest_index]"
      ],
      "metadata": {
        "id": "mHrU7hON38n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define X and y\n",
        "X = train_features\n",
        "y = train_labels\n",
        "\n",
        "selected_feature_indices = optimize_feature_subset(train_features, train_labels)\n",
        "\n",
        "##############################################################################\n",
        "\n",
        "selected_train_features = train_features[:, selected_feature_indices]\n",
        "selected_test_features = test_features[:, selected_feature_indices]\n",
        "\n",
        "# Check the shape of the feature arrays\n",
        "print(\"Shape of train_features:\", selected_train_features.shape)\n",
        "print(\"Shape of test_features:\", selected_test_features.shape)\n",
        "\n",
        "###############################################################################"
      ],
      "metadata": {
        "id": "R1mxN4d638n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ga_scores = acc_score(selected_train_features, selected_test_features, train_labels, test_labels)"
      ],
      "metadata": {
        "id": "j862MyVL38n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sorting by column 'Classifier'\n",
        "ga_sorted_scores = ga_scores.sort_values(by=['F1_Score'], ascending=False)\n",
        "print(\"Results on test data\")\n",
        "print(\"-----------------------------\")\n",
        "\n",
        "ga_sorted_scores.head(10)"
      ],
      "metadata": {
        "id": "wYlFS6v638n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ga_sorted_scores.to_csv(\"Classification with Ensemble Model After Applying GA.csv\")"
      ],
      "metadata": {
        "id": "xepny9HF79wd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}